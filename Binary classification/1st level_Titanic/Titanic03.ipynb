{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Titanic03.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyaKUQd/YrP2qg05y+/+QB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3twSZxUc0M-Q","colab_type":"text"},"source":["* 이 커널은 [Titanic Top 4% with ensemble modeling](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling) 커널을 필사하며 정리한 것입니다.\n","\n","# Titanic Top 4% with ensemble modeling\n","\n","* **1 Introduction**\n","\n","* **2 Load and check data**<br>\n"," * 2.1 load data\n"," * 2.2 Outlier detection\n"," * 2.3 joining train and test set\n"," * 2.4 check for null and missing values\n","\n","* **3 Feature analysis**\n"," * 3.1 Numerical values\n"," * 3.2 Categorical values\n","\n","* **4 Filling missing Values**\n"," * 4.1 Age\n","\n","* **5 Feature engineering**\n"," * 5.1 Name/Title\n"," * 5.2 Family Size\n"," * 5.3 Cabin\n"," * 5.4 Ticket\n","\n","* **6 Modeling**\n"," * 6.1 Simple modeling\n","   * 6.1.1 Cross validate models\n","   * 6.1.2 Hyperparamater tunning for best models\n","   * 6.1.3 Plot learning curves\n","   * 6.1.4 Feature importance of the tree based classifiers\n"," * 6.2 Ensemble modeling\n","   * 6.2.1 Combining models\n"," * 6.3 Prediction\n","   * 6.3.1 Predict and Submit results\n","\n","<br><br>\n","\n","## 1. Introduction\n","이 커널은 크게 세 가지 파트로 나뉩니다.\n","\n","* **Feature analysis**\n","* **Feature engineering**\n","* **Modeling**"]},{"cell_type":"code","metadata":{"id":"UeN3KVhXDhWu","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","from collections import Counter\n","\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n","\n","sns.set(style='white', context='notebook', palette='deep')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fzVYRVqwFTbB","colab_type":"text"},"source":["## 2. Load and check data\n","### 2.1 Load data"]},{"cell_type":"code","metadata":{"id":"WGCf_LjoFSXt","colab_type":"code","colab":{}},"source":["train = pd.read_csv(\"/content/train.csv\")\n","test = pd.read_csv(\"/content/test.csv\")\n","IDtest = test[\"PassengerId\"]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqOCbkzOFhOx","colab_type":"text"},"source":["### 2.2 Outlier detection"]},{"cell_type":"code","metadata":{"id":"r10tuamOFjN5","colab_type":"code","colab":{}},"source":["# Outlier detection \n","\n","def detect_outliers(df,n,features):\n","    \"\"\"\n","    Takes a dataframe df of features and returns a list of the indices\n","    corresponding to the observations containing more than n outliers according\n","    to the Tukey method.\n","    \"\"\"\n","    outlier_indices = []\n","    \n","    # iterate over features(columns)\n","    for col in features:\n","        # 1st quartile (25%)\n","        Q1 = np.percentile(df[col], 25)\n","        # 3rd quartile (75%)\n","        Q3 = np.percentile(df[col],75)\n","        # Interquartile range (IQR)\n","        IQR = Q3 - Q1\n","        \n","        # outlier step\n","        outlier_step = 1.5 * IQR\n","        \n","        # Determine a list of indices of outliers for feature col\n","        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n","        \n","        # append the found outlier indices for col to the list of outlier indices \n","        outlier_indices.extend(outlier_list_col)\n","        \n","    # select observations containing more than 2 outliers\n","    outlier_indices = Counter(outlier_indices)        \n","    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n","    \n","    return multiple_outliers   \n","\n","# detect outliers from Age, SibSp , Parch and Fare\n","Outliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])"],"execution_count":0,"outputs":[]}]}